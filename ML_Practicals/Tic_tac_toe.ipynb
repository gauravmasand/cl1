{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b56a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5990cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.current_player = 1  # 1 for X, -1 for O\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the game board\"\"\"\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.current_player = 1\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"Return current board state as a tuple (for hashing)\"\"\"\n",
    "        return tuple(self.board.flatten())\n",
    "    \n",
    "    def available_actions(self):\n",
    "        \"\"\"Return list of available positions\"\"\"\n",
    "        return [(i, j) for i in range(3) for j in range(3) if self.board[i][j] == 0]\n",
    "    \n",
    "    def make_move(self, position):\n",
    "        \"\"\"Make a move and return (next_state, reward, done)\"\"\"\n",
    "        if self.board[position] != 0:\n",
    "            return self.get_state(), -10, True  # Invalid move penalty\n",
    "        \n",
    "        self.board[position] = self.current_player\n",
    "        \n",
    "        # Check for win\n",
    "        if self.check_winner(self.current_player):\n",
    "            return self.get_state(), 1, True  # Win\n",
    "        \n",
    "        # Check for draw\n",
    "        if len(self.available_actions()) == 0:\n",
    "            return self.get_state(), 0.5, True  # Draw\n",
    "        \n",
    "        # Switch player\n",
    "        self.current_player *= -1\n",
    "        return self.get_state(), 0, False  # Continue game\n",
    "    \n",
    "    def check_winner(self, player):\n",
    "        \"\"\"Check if the given player has won\"\"\"\n",
    "        # Check rows\n",
    "        for i in range(3):\n",
    "            if np.all(self.board[i, :] == player):\n",
    "                return True\n",
    "        \n",
    "        # Check columns\n",
    "        for j in range(3):\n",
    "            if np.all(self.board[:, j] == player):\n",
    "                return True\n",
    "        \n",
    "        # Check diagonals\n",
    "        if np.all(np.diag(self.board) == player):\n",
    "            return True\n",
    "        if np.all(np.diag(np.fliplr(self.board)) == player):\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the board\"\"\"\n",
    "        symbols = {0: '.', 1: 'X', -1: 'O'}\n",
    "        for row in self.board:\n",
    "            print(' '.join([symbols[cell] for cell in row]))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfab0015",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, player_id=1, alpha=0.1, gamma=0.9, epsilon=0.2):\n",
    "        \"\"\"\n",
    "        Q-Learning Agent for Tic-Tac-Toe\n",
    "        \n",
    "        Parameters:\n",
    "        - player_id: 1 for X, -1 for O\n",
    "        - alpha: learning rate\n",
    "        - gamma: discount factor\n",
    "        - epsilon: exploration rate for epsilon-greedy policy\n",
    "        \"\"\"\n",
    "        self.player_id = player_id\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.q_table = defaultdict(float)  # Q-value table\n",
    "        self.state_history = []  # Store states during episode\n",
    "        \n",
    "    def get_q_value(self, state, action):\n",
    "        \"\"\"Get Q-value for state-action pair\"\"\"\n",
    "        return self.q_table[(state, action)]\n",
    "    \n",
    "    def choose_action(self, game, training=True):\n",
    "        \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "        available = game.available_actions()\n",
    "        \n",
    "        if not available:\n",
    "            return None\n",
    "        \n",
    "        state = game.get_state()\n",
    "        \n",
    "        # Epsilon-greedy: explore vs exploit\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.choice(available)  # Explore\n",
    "        \n",
    "        # Exploit: choose action with highest Q-value\n",
    "        q_values = [self.get_q_value(state, action) for action in available]\n",
    "        max_q = max(q_values)\n",
    "        \n",
    "        # If multiple actions have same max Q-value, choose randomly among them\n",
    "        best_actions = [action for action, q in zip(available, q_values) if q == max_q]\n",
    "        return random.choice(best_actions)\n",
    "    \n",
    "    def update_q_value(self, state, action, reward, next_state, next_available_actions):\n",
    "        \"\"\"Update Q-value using Q-learning formula\"\"\"\n",
    "        current_q = self.get_q_value(state, action)\n",
    "        \n",
    "        # Calculate max Q-value for next state\n",
    "        if next_available_actions:\n",
    "            max_next_q = max([self.get_q_value(next_state, a) for a in next_available_actions])\n",
    "        else:\n",
    "            max_next_q = 0\n",
    "        \n",
    "        # Q-learning update rule: Q(s,a) = Q(s,a) + Î±[r + Î³*max(Q(s',a')) - Q(s,a)]\n",
    "        new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)\n",
    "        self.q_table[(state, action)] = new_q\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        \"\"\"Save Q-table to file\"\"\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(dict(self.q_table), f)\n",
    "    \n",
    "    def load_model(self, filename):\n",
    "        \"\"\"Load Q-table from file\"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            self.q_table = defaultdict(float, pickle.load(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cca7acf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Q-Learning agents...\n",
      "\n",
      "Episode 10000/50000\n",
      "  X wins: 6379 (63.8%)\n",
      "  O wins: 2852 (28.5%)\n",
      "  Draws: 769 (7.7%)\n",
      "  Q-table size: 13751\n",
      "\n",
      "Episode 20000/50000\n",
      "  X wins: 13027 (65.1%)\n",
      "  O wins: 5619 (28.1%)\n",
      "  Draws: 1354 (6.8%)\n",
      "  Q-table size: 14371\n",
      "\n",
      "Episode 30000/50000\n",
      "  X wins: 19670 (65.6%)\n",
      "  O wins: 8434 (28.1%)\n",
      "  Draws: 1896 (6.3%)\n",
      "  Q-table size: 14548\n",
      "\n",
      "Episode 40000/50000\n",
      "  X wins: 26399 (66.0%)\n",
      "  O wins: 11205 (28.0%)\n",
      "  Draws: 2396 (6.0%)\n",
      "  Q-table size: 14626\n",
      "\n",
      "Episode 50000/50000\n",
      "  X wins: 33077 (66.2%)\n",
      "  O wins: 13983 (28.0%)\n",
      "  Draws: 2940 (5.9%)\n",
      "  Q-table size: 14670\n",
      "\n",
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "def train_agents(episodes=50000, verbose=True):\n",
    "    \"\"\"\n",
    "    Train two Q-learning agents by playing against each other\n",
    "    \"\"\"\n",
    "    game = TicTacToe()\n",
    "    agent1 = QLearningAgent(player_id=1, alpha=0.1, gamma=0.9, epsilon=0.2)\n",
    "    agent2 = QLearningAgent(player_id=-1, alpha=0.1, gamma=0.9, epsilon=0.2)\n",
    "    \n",
    "    wins = {1: 0, -1: 0, 0: 0}  # Track wins for X, O, and draws\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = game.reset()\n",
    "        episode_history = []  # Store (agent, state, action) for this episode\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            current_agent = agent1 if game.current_player == 1 else agent2\n",
    "            \n",
    "            # Choose and make action\n",
    "            action = current_agent.choose_action(game, training=True)\n",
    "            if action is None:\n",
    "                break\n",
    "            \n",
    "            prev_state = game.get_state()\n",
    "            next_state, reward, done = game.make_move(action)\n",
    "            \n",
    "            # Store for later update\n",
    "            episode_history.append((current_agent, prev_state, action, reward, next_state, done))\n",
    "        \n",
    "        # Determine winner\n",
    "        if game.check_winner(1):\n",
    "            winner = 1\n",
    "            wins[1] += 1\n",
    "        elif game.check_winner(-1):\n",
    "            winner = -1\n",
    "            wins[-1] += 1\n",
    "        else:\n",
    "            winner = 0\n",
    "            wins[0] += 1\n",
    "        \n",
    "        # Update Q-values for both agents\n",
    "        for i, (agent, state, action, reward, next_state, done) in enumerate(episode_history):\n",
    "            # Adjust reward based on outcome\n",
    "            if done:\n",
    "                if game.check_winner(agent.player_id):\n",
    "                    final_reward = 1  # Win\n",
    "                elif game.check_winner(-agent.player_id):\n",
    "                    final_reward = -1  # Loss\n",
    "                else:\n",
    "                    final_reward = 0.5  # Draw\n",
    "            else:\n",
    "                final_reward = 0  # Intermediate move\n",
    "            \n",
    "            next_available = game.available_actions() if not done else []\n",
    "            agent.update_q_value(state, action, final_reward, next_state, next_available)\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose and (episode + 1) % 10000 == 0:\n",
    "            total = episode + 1\n",
    "            print(f\"Episode {episode + 1}/{episodes}\")\n",
    "            print(f\"  X wins: {wins[1]} ({wins[1]/total*100:.1f}%)\")\n",
    "            print(f\"  O wins: {wins[-1]} ({wins[-1]/total*100:.1f}%)\")\n",
    "            print(f\"  Draws: {wins[0]} ({wins[0]/total*100:.1f}%)\")\n",
    "            print(f\"  Q-table size: {len(agent1.q_table)}\")\n",
    "            print()\n",
    "    \n",
    "    return agent1, agent2, wins\n",
    "\n",
    "# Train the agents\n",
    "print(\"Training Q-Learning agents...\\n\")\n",
    "trained_agent1, trained_agent2, training_stats = train_agents(episodes=50000)\n",
    "print(\"\\nTraining completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "619fa8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results (Agent as X vs Random Player):\n",
      "  X wins: 833 (83.3%)\n",
      "  O wins: 110 (11.0%)\n",
      "  Draws: 57 (5.7%)\n"
     ]
    }
   ],
   "source": [
    "def test_against_random(agent, num_games=100, agent_player=1):\n",
    "    \"\"\"\n",
    "    Test trained agent against random player\n",
    "    \"\"\"\n",
    "    game = TicTacToe()\n",
    "    wins = {1: 0, -1: 0, 0: 0}\n",
    "    \n",
    "    for _ in range(num_games):\n",
    "        state = game.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            if game.current_player == agent_player:\n",
    "                # Trained agent's turn\n",
    "                action = agent.choose_action(game, training=False)\n",
    "            else:\n",
    "                # Random player's turn\n",
    "                available = game.available_actions()\n",
    "                action = random.choice(available) if available else None\n",
    "            \n",
    "            if action is None:\n",
    "                break\n",
    "            \n",
    "            next_state, reward, done = game.make_move(action)\n",
    "        \n",
    "        # Record result\n",
    "        if game.check_winner(1):\n",
    "            wins[1] += 1\n",
    "        elif game.check_winner(-1):\n",
    "            wins[-1] += 1\n",
    "        else:\n",
    "            wins[0] += 1\n",
    "    \n",
    "    print(f\"\\nTest Results (Agent as {'X' if agent_player == 1 else 'O'} vs Random Player):\")\n",
    "    print(f\"  X wins: {wins[1]} ({wins[1]/num_games*100:.1f}%)\")\n",
    "    print(f\"  O wins: {wins[-1]} ({wins[-1]/num_games*100:.1f}%)\")\n",
    "    print(f\"  Draws: {wins[0]} ({wins[0]/num_games*100:.1f}%)\")\n",
    "    \n",
    "    return wins\n",
    "\n",
    "# Test the trained agent\n",
    "test_results = test_against_random(trained_agent1, num_games=1000, agent_player=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57a4a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_against_human(agent, agent_player=1):\n",
    "    \"\"\"\n",
    "    Play Tic-Tac-Toe against the trained agent\n",
    "    \"\"\"\n",
    "    game = TicTacToe()\n",
    "    state = game.reset()\n",
    "    \n",
    "    print(\"\\nTic-Tac-Toe: You vs AI\")\n",
    "    print(f\"You are {'O' if agent_player == 1 else 'X'}\")\n",
    "    print(f\"AI is {'X' if agent_player == 1 else 'O'}\")\n",
    "    print(\"\\nEnter moves as 'row,col' (0-2), e.g., '0,1' for top-middle\\n\")\n",
    "    \n",
    "    game.display()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if game.current_player == agent_player:\n",
    "            # AI's turn\n",
    "            print(\"AI is thinking...\")\n",
    "            action = agent.choose_action(game, training=False)\n",
    "            print(f\"AI plays: {action}\")\n",
    "        else:\n",
    "            # Human's turn\n",
    "            while True:\n",
    "                try:\n",
    "                    move = input(\"Your move (row,col): \")\n",
    "                    row, col = map(int, move.split(','))\n",
    "                    if (row, col) in game.available_actions():\n",
    "                        action = (row, col)\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Invalid move! That position is taken.\")\n",
    "                except:\n",
    "                    print(\"Invalid input! Use format: row,col (e.g., 1,2)\")\n",
    "        \n",
    "        next_state, reward, done = game.make_move(action)\n",
    "        game.display()\n",
    "    \n",
    "    # Announce winner\n",
    "    if game.check_winner(1):\n",
    "        winner = \"X\"\n",
    "    elif game.check_winner(-1):\n",
    "        winner = \"O\"\n",
    "    else:\n",
    "        winner = None\n",
    "    \n",
    "    if winner:\n",
    "        if (winner == 'X' and agent_player == 1) or (winner == 'O' and agent_player == -1):\n",
    "            print(\"ðŸ¤– AI wins!\")\n",
    "        else:\n",
    "            print(\"ðŸŽ‰ You win!\")\n",
    "    else:\n",
    "        print(\"ðŸ¤ It's a draw!\")\n",
    "\n",
    "# Play a game (uncomment to play)\n",
    "# play_against_human(trained_agent1, agent_player=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d67b528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "AGENT EVALUATION\n",
      "==================================================\n",
      "\n",
      "1. Agent playing as X (first):\n",
      "\n",
      "Test Results (Agent as X vs Random Player):\n",
      "  X wins: 819 (81.9%)\n",
      "  O wins: 118 (11.8%)\n",
      "  Draws: 63 (6.3%)\n",
      "\n",
      "2. Agent playing as O (second):\n",
      "\n",
      "Test Results (Agent as O vs Random Player):\n",
      "  X wins: 575 (57.5%)\n",
      "  O wins: 298 (29.8%)\n",
      "  Draws: 127 (12.7%)\n",
      "\n",
      "==================================================\n",
      "Q-table contains 15453 state-action pairs\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def evaluate_agent_strength(agent, num_games=100):\n",
    "    \"\"\"\n",
    "    Evaluate agent by playing against random player from both positions\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"AGENT EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test as X (first player)\n",
    "    print(\"\\n1. Agent playing as X (first):\")\n",
    "    results_x = test_against_random(agent, num_games=num_games, agent_player=1)\n",
    "    \n",
    "    # Test as O (second player)\n",
    "    print(\"\\n2. Agent playing as O (second):\")\n",
    "    results_o = test_against_random(agent, num_games=num_games, agent_player=-1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Q-table contains {len(agent.q_table)} state-action pairs\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "evaluate_agent_strength(trained_agent1, num_games=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae1dbad",
   "metadata": {},
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f0cffcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 1000 games vs random: Wins=660, Draws=40, Losses=300\n"
     ]
    }
   ],
   "source": [
    "# Simple Tic-Tac-Toe with Q-learning\n",
    "# This code is intentionally compact and easy to memorize.\n",
    "# It trains an agent (X) to play against a random opponent (O).\n",
    "# Run in one cell (e.g. Google Colab). No external libs except numpy.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = [' ']*9\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        return self._state()\n",
    "\n",
    "    def _state(self):\n",
    "        return ''.join(self.board)\n",
    "\n",
    "    def available_actions(self):\n",
    "        return [i for i, v in enumerate(self.board) if v == ' ']\n",
    "\n",
    "    def step(self, action, mark):\n",
    "        if self.board[action] != ' ':\n",
    "            raise ValueError(\"Invalid action\")\n",
    "        self.board[action] = mark\n",
    "        self._check_game()\n",
    "        return self._state(), self._reward(mark), self.done\n",
    "\n",
    "    def _check_game(self):\n",
    "        b = self.board\n",
    "        lines = [(0,1,2),(3,4,5),(6,7,8),(0,3,6),(1,4,7),(2,5,8),(0,4,8),(2,4,6)]\n",
    "        for a,b1,c in lines:\n",
    "            if self.board[a]==self.board[b1]==self.board[c] != ' ':\n",
    "                self.done = True\n",
    "                self.winner = self.board[a]\n",
    "                return\n",
    "        if ' ' not in self.board:\n",
    "            self.done = True\n",
    "            self.winner = None\n",
    "\n",
    "    def _reward(self, last_mark):\n",
    "        if not self.done:\n",
    "            return 0\n",
    "        if self.winner is None:\n",
    "            return 0.5  # small reward for draw\n",
    "        return 1 if self.winner == last_mark else -1\n",
    "\n",
    "# Q-learning agent\n",
    "class QAgent:\n",
    "    def __init__(self, lr=0.5, gamma=0.9, eps=0.2):\n",
    "        self.q = defaultdict(lambda: np.zeros(9))\n",
    "        self.lr = lr; self.gamma=gamma; self.eps=eps\n",
    "\n",
    "    def choose(self, state, available):\n",
    "        if random.random() < self.eps:\n",
    "            return random.choice(available)\n",
    "        qvals = self.q[state].copy()\n",
    "        # mask unavailable actions very low\n",
    "        for a in range(9):\n",
    "            if a not in available:\n",
    "                qvals[a] = -999\n",
    "        return int(np.argmax(qvals))\n",
    "\n",
    "    def learn(self, s, a, r, s2, available2):\n",
    "        qsa = self.q[s][a]\n",
    "        if s2 is None:\n",
    "            target = r\n",
    "        else:\n",
    "            # value for next state (only consider available actions)\n",
    "            next_q = self.q[s2].copy()\n",
    "            for idx in range(9):\n",
    "                if idx not in available2:\n",
    "                    next_q[idx] = -999\n",
    "            target = r + self.gamma * np.max(next_q)\n",
    "        self.q[s][a] += self.lr * (target - qsa)\n",
    "\n",
    "# Training: agent plays as 'X' and learns, opponent plays random 'O'\n",
    "def train(episodes=5000):\n",
    "    env = TicTacToe()\n",
    "    agent = QAgent(lr=0.5, gamma=0.9, eps=0.3)\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        player = 'X' if random.random() < 0.5 else 'O'\n",
    "        last = None  # store (s,a,mark)\n",
    "        while True:\n",
    "            if player == 'X':\n",
    "                avail = env.available_actions()\n",
    "                a = agent.choose(state, avail)\n",
    "                s2, r, done = env.step(a, 'X')\n",
    "                avail2 = env.available_actions() if not done else []\n",
    "                # learn from X's move\n",
    "                agent.learn(state, a, r, s2 if not done else None, avail2)\n",
    "                state = s2\n",
    "                if done: break\n",
    "                player = 'O'\n",
    "            else:\n",
    "                # random opponent\n",
    "                avail = env.available_actions()\n",
    "                a = random.choice(avail)\n",
    "                s2, r, done = env.step(a, 'O')\n",
    "                # penalize agent if O wins on this move\n",
    "                if done:\n",
    "                    # if O won, give negative reward to previous X move (handled above when X moved)\n",
    "                    pass\n",
    "                state = s2\n",
    "                if done: break\n",
    "                player = 'X'\n",
    "        # slowly decay epsilon\n",
    "        if agent.eps > 0.05:\n",
    "            agent.eps *= 0.9995\n",
    "    return agent\n",
    "\n",
    "def evaluate(agent, games=1000):\n",
    "    env = TicTacToe()\n",
    "    wins = draws = losses = 0\n",
    "    for _ in range(games):\n",
    "        state = env.reset()\n",
    "        player = 'X' if random.random() < 0.5 else 'O'\n",
    "        while True:\n",
    "            if player == 'X':\n",
    "                a = agent.choose(state, env.available_actions())\n",
    "                state, r, done = env.step(a, 'X')\n",
    "                if done:\n",
    "                    if env.winner == 'X': wins += 1\n",
    "                    elif env.winner is None: draws += 1\n",
    "                    else: losses += 1\n",
    "                    break\n",
    "                player = 'O'\n",
    "            else:\n",
    "                a = random.choice(env.available_actions())\n",
    "                state, r, done = env.step(a, 'O')\n",
    "                if done:\n",
    "                    if env.winner == 'X': wins += 1\n",
    "                    elif env.winner is None: draws += 1\n",
    "                    else: losses += 1\n",
    "                    break\n",
    "                player = 'X'\n",
    "    return wins, draws, losses\n",
    "\n",
    "# Run training and evaluation\n",
    "agent = train(episodes=8000)\n",
    "w,d,l = evaluate(agent, games=1000)\n",
    "print(f\"Out of 1000 games vs random: Wins={w}, Draws={d}, Losses={l}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6e265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
